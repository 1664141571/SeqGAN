{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第6回講義　演習　SeqGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. 導入"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1. 強化学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "強化学習では、\n",
    "\n",
    "1. 時刻tにおいて、状態(state):$s_t$にいるエージェント(agent)が行動(action):$a_t$をとる\n",
    "1. エージェントは行動$a_t$によって状態$s_{t+1}$に遷移し、即時報酬(immediate reward):$r_{t+1}$を受け取る\n",
    "1. 以上を繰り返す\n",
    "\n",
    "という一連の流れでエージェントが得る割引報酬の和\n",
    "\n",
    "$R=\\sum_{t=0}^∞ \\gamma^t r_{t+1}$\n",
    "\n",
    "を最大化するようにエージェントを学習させることが目的です。\n",
    "\n",
    "また終端状態(terminal state): $s_T$が存在する様なタスクはepisodicであるといい、初期状態から終端状態までをepisodeと呼びます。episodic taskの時のみ割引率$\\gamma$は1にできます。\n",
    "\n",
    "例えば囲碁のようなゲームはepisodicです。\n",
    "\n",
    "この演習ではepisodic taskを扱うとし、割引率は$\\gamma$は1、初期状態を$s_0$、最終時刻を$t=T$と定めます。\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "他の重要な構成要素\n",
    "\n",
    "* **方策(Policy)**: ある状態sでのエージェントがとる行動aを決める関数\n",
    "    * Deterministic policy: ある状態sでは必ず行動aをとる\n",
    "        * $a = \\pi (s)$\n",
    "    * Stochastic policy: ある状態sで行動aをとる確率を定義 （← SeqGANはこっち）\n",
    "        * $\\pi (a|s)$\n",
    "* **価値関数(Value function)**:\n",
    "    * $V(s)$, ある状態s以降における(割引)報酬の和の期待値\n",
    "* **行動価値関数(Action-value function)**\n",
    "    * $Q(s, a)$, ある状態sで行動aを取ったあとの(割引)報酬の和の期待値\n",
    "\n",
    "これらの定義より、\n",
    "\\begin{align}\n",
    "V(s_{t})=\\sum_{a_t} \\pi (a_t|s_t) Q(s_t, a_t)\\\\\n",
    "Q(s_t, a_t)=r(s, a)+V(s_{t+1})\n",
    "\\end{align}\n",
    "\n",
    "と書ける。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "強化学習の手法を大別\n",
    "\n",
    "* Value based RL\n",
    "    * Value function$V$またはAction-value function $Q$をパラメータで記述し、最適なパラメータを探す問題に帰着\n",
    "    * 例：TD法、Q-learning, DQN, etc.\n",
    "    * 利点：未知の状態$s$にも対応可能\n",
    "* Policy based RL\n",
    "    * Policy $\\pi$をパラメータで記述し、以下同文\n",
    "    * 例：Policy gradient\n",
    "    * 利点：Stochastic policyも学習可能\n",
    "\n",
    "まとめると、\n",
    "\n",
    "|種別|パラメータで記述するもの|利点|アルゴリズム例|\n",
    "|---|---|---|---|\n",
    "|Value based RL|Value function $V$ または $Q$|未知の状態 s にも対応可能|TD法、Q-learning, DQN, etc.|\n",
    "|Policy based RL|Policy $\\pi$|Stochastic policyも学習可能|Policy gradient|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2. SeqGANにおける強化学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./image/seqgan.png\" width=600>\n",
    "<div style=\"text-align: center;\">\n",
    "出典:[SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient](https://arxiv.org/abs/1609.05473)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SeqGANとは列(sequence)の生成を行うためのGANです。教師あり学習だけではなく、強化学習の1手法であるPolicy gradientを用いて学習させます。\n",
    "\n",
    "#### 0.2.1. なぜ強化学習を用いる必要があったのか\n",
    "\n",
    "Teacher Forcingによる最尤推定(MLE)でも列生成モデルは学習することができます。しかしRNNをMLEで学習すると、訓練時はターゲット系列をそのまま入力として学習するがテスト時は自分の出力が次の時刻での入力となるために訓練時に見たことのない入力がテスト時に出てくる問題(_\"exposure bias\"_)が発生する可能性が高いです。この問題への対策の一つが訓練時にも一定の確率で自分の出力を次の時刻の入力とする手法がscheduled sampling (Bengio et al., 2015) でした。（第３回演習参照）しかしこれは本質的な問題の解決にはならないという報告もあります(Husz´ar, 2015)。\n",
    "\n",
    "新たな解決策として、生成モデルの学習方法であるGAN (Generative Adversarial Network) の枠組みを文生成に適用し、さらに出力が離散値であるテキスト生成でも強化学習による学習を可能にしたのがこの論文の新規性です。\n",
    "\n",
    "これによって、教師あり学習のみの場合よりも、よりTrue dataと似たような列を生成することができます。\n",
    "例えば大量の詩のデータをTrue dataとしてSeqGANに学習させれば、それに似た新たな詩を生成することができます。データは列であれば良いので、音楽にも適用可能です。\n",
    "\n",
    "#### 0.2.2. SeqGANにおける強化学習\n",
    "\n",
    "SeqGANの実装における構成要素は以下のようになっています。\n",
    "\n",
    "* True data\n",
    "    * 実世界に存在するデータ。この演習では夏目漱石の『こころ』を扱います。\n",
    "\n",
    "\n",
    "* Generator: G\n",
    "    * LSTMからなるモデルです。Generated dataを出力します。True dataと似たデータをGeneratorが出力できるようにすることが目的です。\n",
    "\n",
    "\n",
    "* Discriminator: D\n",
    "    * CNNベースの2値分類モデルです。dataがTrue dataなのかGenerated dataなのかを見分けるためのモデルです。\n",
    "\n",
    "\n",
    "* ROLLOUT: モンテカルロ探索 → 報酬計算\n",
    "    * Generator Gを使って各時刻でモンテカルロ探索を行い、出力候補を多数生成（=rollout）します。そしてそれぞれの出力候補に対して、True dataである確率をDiscriminatorが計算し、それを報酬としてGeneratorに渡すことで強化学習を行います。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これらを用いて列（文）生成を強化学習の枠組みに当てはめると以下のようになります。\n",
    "\n",
    "* 文には終端状態があるのでepisodicである\n",
    "* 系列は$Y_{1:T}=(y_1,...,y_t,...,y_T),~y_t \\in {\\mathcal Y}$（${\\mathcal Y}$ は語彙の集合）と表す\n",
    "* 各時刻に1つの単語を生成する\n",
    "* 初期状態$s_0$は何も単語を生成していない状態\n",
    "* 状態$s_t$は単語列$Y_{1:t}$に相当\n",
    "* 行動$a_t$は単語$y_{t+1}$を出力すること\n",
    "* Generator $G_{\\theta}(y_t|Y_{1:t-1})$\n",
    "* 文がTrue dataである確率 $D_\\phi (Y_{1:T})$をDiscriminatorが予測し、それを最終状態$s_T=Y_{1:T}$に対する報酬 $r_T$ とする。\n",
    "* 報酬を計算するDiscriminatorの性質上、即時報酬は$r_t = 0~(t < T)$\n",
    "\n",
    "文生成において、GeneratorをPolicy $\\pi$ とみなしてパラメータ$\\theta$で記述したいので、Policy gradientによって強化学習を行います。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.3. Policy Gradient （方策勾配法）とは"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一言で言うとPolicyをパラメータで記述し、勾配法によって最適なPolicyを探すことを指します。\n",
    "\n",
    "ある状態$s_t=Y_{1:t-1}$でどの行動$a_t=y_t$をとるか(方策, policy)を決める方策関数(policy function)、$\\pi(a_t|s_t;\\theta)=G(y_t|Y_{1:t-1})$を考えます。$\\theta$はパラメータです。\n",
    "\n",
    "この方策関数$\\pi$に従った時の最終的な報酬の和の期待値$V(s_0)$$(=J(\\theta))$を勾配法で最大化する手法が方策勾配法(policy gradient)です。\n",
    "\n",
    "以下のように$\\theta$を更新していくことで方策関数を更新して行き、期待報酬を最大化します。\n",
    "\n",
    "$\\theta \\gets \\theta + \\alpha \\nabla J(\\theta)$\n",
    "\n",
    "$\\alpha$は定数（$\\alpha > 0$）です。\n",
    "\n",
    "$\\nabla J(\\theta)$は以下のように表せます。\n",
    "まずPolicy Gradient Theorem (Sutton et al., 2000) により、\n",
    "\n",
    "\\begin{align*}\n",
    "& \\nabla J(\\theta) = \\nabla V(s_0) \\\\\n",
    "& = \\nabla \\left[ \\sum_{y_1 \\in {\\mathcal Y}} G_\\theta (y_1|s_0) Q(s_0, y_1) \\right] \\\\\n",
    "& = \\sum_{t=1}^T \\sum_{Y_{1:t-1}} P(Y_{1:t-1}|s_0;G_\\theta) \\sum_{y_t \\in {\\mathcal Y}} \\nabla G_\\theta (y_t|Y_{1:t-1}) Q(Y_{1:t-1}, y_t) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "$\\nabla f = f \\nabla \\log f$より、\n",
    "\n",
    "\\begin{align*}\n",
    "& = \\sum_{t=1}^T \\sum_{Y_{1:t-1}} P(Y_{1:t-1}|s_0;G_\\theta) \\sum_{y_t \\in {\\mathcal Y}} G_\\theta (y_t|Y_{1:t-1}) \\nabla \\log G_\\theta (y_t|Y_{1:t-1}) Q(Y_{1:t-1}, y_t) \\\\\n",
    "& = \\sum_{t=1}^T \\sum_{Y_{1:t}} P(Y_{1:t}|s_0;G_\\theta)  \\nabla \\log G_\\theta (y_t|Y_{1:t-1}) Q(Y_{1:t-1}, y_t) \\\\\n",
    "& = \\sum_{t=1}^T {\\mathbb E}_{Y_{1:t} \\sim G_\\theta} \\left[ \\nabla \\log G_\\theta (y_t|Y_{1:t-1}) Q(Y_{1:t-1}, y_t) \\right]\n",
    "\\end{align*}\n",
    "\n",
    "※詳しい証明は[論文](https://arxiv.org/pdf/1609.05473.pdf)のAppendix参照"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.4. Monte Carlo Policy Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.3.のように、$\\nabla J(\\theta)$を計算できれば良いのですが、2つ問題があります。1つ目は${\\mathbb E}_{Y_{1:t} \\sim G_\\theta} \\left[ \\cdot \\right]$をどう計算するか、2つ目はどうやって$Q(Y_{1:t-1}, y_t)$を計算するかです。\n",
    "\n",
    "1つ目の${\\mathbb E}_{Y_{1:t} \\sim G_\\theta} \\left[ \\cdot \\right]$はPolicy $G_\\theta$を使って$Y_{1:t}$をサンプリングすることで推定できます。\n",
    "\n",
    "2つ目の$Q(Y_{1:t-1}, y_t)$は、Monte Carlo法で推定します。Discriminatorは文全体$Y_{1:T}$に対してしか報酬を与えることができないので、$Y_{t+1:T}$をRollout policy $G_\\beta$を使ってランダムにN回サンプリングして得られた結果$\\{ Y^1_{1:T},...,Y^N_{1:T}\\} = MC^{G_\\beta}(Y_{1:t};N)$への報酬の平均を$Q(Y_{1:t-1}, y_t)$とします。こうすることで中間の状態に対しても$Q$を定義できます。\n",
    "\n",
    "\\begin{align}\n",
    "&Q(Y_{1:t-1}, y_t)=\\\\\n",
    "&\\begin{cases}\n",
    "\\frac{1}{N}\\sum_{n=1}^ND_\\phi(Y^n_{1:T}),~Y^n_{1:T} \\in MC^{G_\\beta}(Y_{1:t};N) & {\\rm for}~ t < T\\\\\n",
    "D_\\phi(Y_{1:t}) & {\\rm for}~t=T\n",
    "\\end{cases}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.4. 学習の流れ\n",
    "\n",
    "学習は以下の流れで行います。\n",
    "* Pretraining（事前学習）\n",
    "  1. GeneratorをTrue dataで事前学習（**教師あり学習**）\n",
    "  1. Discriminatorを教師あり学習\n",
    "\n",
    "を行なった後に\n",
    "\n",
    "* Adversarial Training（敵対的学習）\n",
    "  1. GeneratorをPolicy Gradientで学習（**強化学習**）\n",
    "  1. Discriminatorを教師あり学習\n",
    "  1. 1に戻る"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f17d4310890>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import copy\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "try:\n",
    "    from utils import Vocab\n",
    "except ModuleNotFoundError:\n",
    "    os.chdir('/root/userspace/chap6/')\n",
    "    from utils import Vocab\n",
    "    \n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ハイパーパラメータ設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator Hyper-parameters\n",
    "G_EMBEDDING_SIZE = 32 # 埋め込みベクトルの次元数\n",
    "G_HIDDEN_SIZE = 32 # LSTMの隠れ状態ベクトルの次元数\n",
    "G_MAX_LENGTH = 40 # 系列の長さ\n",
    "# G_PRE_NUM_EPOCHS = 120 # 事前学習を行うエポック数\n",
    "G_PRE_NUM_EPOCHS = 1\n",
    "G_BATCH_SIZE = 64 # バッチサイズ\n",
    "\n",
    "# Discriminator Hyper-parameters\n",
    "D_EMBEDDING_SIZE = 64\n",
    "D_FILTER_SIZES = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20] # CNNに使うフィルターのサイズ\n",
    "D_NUM_FILTERS = [100, 200, 200, 200, 200, 100, 100, 100, 100, 100, 160, 160] # CNNに使うフィルターの数\n",
    "D_DROPOUT_PROB = 0.75 # Dropoutの確率\n",
    "# D_PRE_NUM_EPOCHS = 50\n",
    "D_PRE_NUM_EPOCHS = 1\n",
    "D_BATCH_SIZE = 64\n",
    "\n",
    "# Basic Training Parameters\n",
    "# ADV_N_BATCHES = 200 # 敵対的学習を行うエポック数\n",
    "ADV_N_BATCHES = 1\n",
    "POSITIVE_FILE = './save/real_data.txt'\n",
    "NEGATIVE_FILE = './save/generator_sample.txt'\n",
    "# GENERATED_NUM = 10000\n",
    "GENERATED_NUM = 1000 # 敵対的学習のためにGeneratorに出力させるサンプル数\n",
    "\n",
    "# 保存用ディレクトリがなければ作成する\n",
    "if not os.path.exists('./save'):\n",
    "    os.mkdir('./save')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. データ\n",
    "夏目漱石の『こころ』を学習データに使います。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. データ読み込み・前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特殊なトークンを事前に定義します\n",
    "PAD_TOKEN = '<PAD>'\n",
    "BOS_TOKEN = '<S>'\n",
    "EOS_TOKEN = '</S>'\n",
    "UNK_TOKEN = '<UNK>'\n",
    "PAD = 0\n",
    "BOS = 1\n",
    "EOS = 2\n",
    "UNK = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    \"\"\"スペース区切りのコーパスを1行ごとに読み込む関数\n",
    "\n",
    "    :path: str, ファイルのパス\n",
    "    :return text: list of list of str\n",
    "    \"\"\"\n",
    "    text = []\n",
    "    with open(path, \"r\") as f:\n",
    "        for line in f:\n",
    "            text.append(line.strip().split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['私', 'は', 'その', '人', 'を', '常に', '先生', 'と', '呼ん', 'で', 'い', 'た', '。']\n"
     ]
    }
   ],
   "source": [
    "# 事前にこちらで分かち書きしたものを用意したのでこれを使ってください。\n",
    "text = load_data(\"./data/kokoro_parsed.txt\")\n",
    "print(text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "語彙数\t: 6661\n"
     ]
    }
   ],
   "source": [
    "MIN_COUNT = 1 # 語彙に含める単語の最低出現回数\n",
    "\n",
    "word2id = {\n",
    "    PAD_TOKEN: PAD,\n",
    "    BOS_TOKEN: BOS,\n",
    "    EOS_TOKEN: EOS,\n",
    "    UNK_TOKEN: UNK,\n",
    "}\n",
    "\n",
    "vocab = Vocab(word2id=word2id)\n",
    "vocab.build_vocab(text, min_count=MIN_COUNT)\n",
    "print(\"語彙数\\t:\", len(vocab.word2id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文の総数\t: 4267\n",
      "文長の最大値\t: 283\n",
      "文長の平均\t: 24.62385751113194\n",
      "標準偏差\t: 17.948284504112998\n"
     ]
    }
   ],
   "source": [
    "# 統計量\n",
    "lens = [len(x) for x in text]\n",
    "print(\"文の総数\\t:\", len(text))\n",
    "print(\"文長の最大値\\t:\", np.max(lens))\n",
    "print(\"文長の平均\\t:\", np.mean(lens))\n",
    "print(\"標準偏差\\t:\", np.std(lens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_ids(vocab, sentence):\n",
    "    \"\"\"単語のリストを単語IDのリストに変換する関数\n",
    "\n",
    "    :param vocab: Vocabのインスタンス\n",
    "    :param sentence: list of str\n",
    "    :return ids: list of int\n",
    "    \"\"\"\n",
    "    ids = [vocab.word2id.get(word, UNK) for word in sentence]\n",
    "    ids = [BOS] + ids + [EOS]  # </S>トークンを末尾に加える\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_seq(sen, max_length):\n",
    "    \"\"\"padding, truncatingを行う\n",
    "\n",
    "    :param sen: list of int, 単語IDのリスト\n",
    "    :param max_length: int, paddingを行なった後のsequenceの長さ\n",
    "    :return sen: list of int, padding後のsequence\n",
    "    \"\"\"\n",
    "    if len(sen) <= max_length:\n",
    "        # Padding\n",
    "        sen += [PAD] * (max_length - len(sen))\n",
    "    else:\n",
    "        # Truncating\n",
    "        sen = sen[:max_length]\n",
    "    return sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 単語をIDに変換して、paddingを行います\n",
    "\n",
    "id_text = []\n",
    "for sen in text:\n",
    "    id_sen = sentence_to_ids(vocab, sen)\n",
    "    id_sen = pad_seq(id_sen, G_MAX_LENGTH)\n",
    "    id_text.append(id_sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IDに変換したデータをファイルに保存します\n",
    "# これを使ってGenerator、Discriminatorを学習させます\n",
    "\n",
    "with open(POSITIVE_FILE, \"w\") as f:\n",
    "    for id_sen in id_text:\n",
    "        f.write(' '.join([str(w) for w in id_sen]) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. データローダー"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generator, Descriminatorそれぞれの学習のために個別にDataLoaderを用意します。\n",
    "ともにデータはtxtファイルから読みます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenDataLoader(object):\n",
    "    \"\"\"Generatorのためのデータローダー\"\"\"\n",
    "    def __init__(self, file_path, batch_size):\n",
    "        \"\"\"\n",
    "        :param file_path: str, 学習用データファイルのパス\n",
    "        :param batch_size: int, ミニバッチのサイズ\n",
    "        \"\"\"\n",
    "        super(GenDataLoader, self).__init__()\n",
    "        self.data = self.load_file(file_path) # ファイル読み込み\n",
    "        self.batch_size = batch_size # バッチサイズ\n",
    "        self.pointer = 0 # ポインター\n",
    "        self.data_num = len(self.data) # データの総数\n",
    "\n",
    "        # データの順番をランダムに並び替える\n",
    "        self.reset()\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.pointer >= self.data_num:\n",
    "            self.reset()\n",
    "            raise StopIteration\n",
    "        batch = torch.tensor(self.data[self.pointer:self.pointer + self.batch_size], dtype=torch.long)\n",
    "        batch_X = batch[:, :-1].to(device) # 入力: <BOS>から<EOS>の手前まで\n",
    "        batch_Y = batch[:, 1:].to(device) # 出力: <BOS>の次から<EOS>まで\n",
    "        self.pointer += self.batch_size\n",
    "        return batch_X, batch_Y\n",
    "\n",
    "    def load_file(self, file_path):\n",
    "        \"\"\"データを読み込むメソッド\n",
    "\n",
    "        :param file_path: str\n",
    "        :return data: list of list of int\n",
    "        \"\"\"\n",
    "        data = []\n",
    "        with open(file_path, \"r\") as f:\n",
    "            for line in f:\n",
    "                line = line.strip().split()\n",
    "                line = [int(x) for x in line]\n",
    "                data.append(line)\n",
    "        return data\n",
    "\n",
    "    def reset(self):\n",
    "        self.pointer = 0\n",
    "        random.shuffle(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DisDataLoader(object):\n",
    "    \"\"\"Discriminatorのためのデータローダー\"\"\"\n",
    "    def __init__(self, positive_file, negative_file, batch_size):\n",
    "        \"\"\"\n",
    "        :param positive_file: str, True Dataのパス\n",
    "        :param negative_file: str, Generated Dataのパス\n",
    "        :param batch_size: int, ミニバッチのサイズ\n",
    "        \"\"\"\n",
    "        super(DisDataLoader, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        pos_data = self.load_file(positive_file) # True data\n",
    "        neg_data = self.load_file(negative_file) # Generated data\n",
    "        self.data = pos_data + neg_data # 入力\n",
    "\n",
    "        # 入力がTrue dataなら出力は1\n",
    "        # 入力がGenerated dataなら出力は0\n",
    "        self.labels = [1 for _ in range(len(pos_data))] + [0 for _ in range(len(neg_data))]\n",
    "        self.pairs = list(zip(self.data, self.labels))\n",
    "        self.data_num = len(self.pairs)\n",
    "        self.pointer = 0\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        if self.pointer >= self.data_num:\n",
    "            self.reset()\n",
    "            raise StopIteration\n",
    "        batch_X, batch_Y = zip(*self.pairs[self.pointer:self.pointer + self.batch_size])\n",
    "        batch_X = torch.tensor(batch_X, dtype=torch.long, device=device) # 入力: True or Generated\n",
    "        batch_Y = torch.tensor(batch_Y, dtype=torch.long, device=device) # 出力: 1 or 0\n",
    "        self.pointer += self.batch_size\n",
    "        return batch_X, batch_Y\n",
    "\n",
    "    def load_file(self, file_path):\n",
    "        \"\"\"データを読み込むメソッド\n",
    "\n",
    "        :param file_path: str\n",
    "        :return data: list of list of int\n",
    "        \"\"\"\n",
    "        data = []\n",
    "        with open(file_path, \"r\") as f:\n",
    "            for line in f:\n",
    "                line = line.strip().split()\n",
    "                line = [int(x) for x in line]\n",
    "                data.append(line)\n",
    "        return data\n",
    "\n",
    "    def reset(self):\n",
    "        self.pointer = 0\n",
    "        random.shuffle(self.pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. モデル定義"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Generator\n",
    "一般的なLSTMを使います。\n",
    "\n",
    "<img src=\"./image/generator.png\" width=720>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, vocab_size, batch_size, embedding_size, hidden_size,\n",
    "                 max_length):\n",
    "        \"\"\"\n",
    "        :param vocab_size: int, 語彙の総数\n",
    "        :param batch_size: int, ミニバッチのサイズ\n",
    "        :param embedding_size: int, 埋め込みベクトルの次元数\n",
    "        :param hidden_size: int, 隠れ状態ベクトルの次元数\n",
    "        :param max_length: int, 入出力系列の長さ\n",
    "        \"\"\"\n",
    "        super(Generator, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.batch_size = batch_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # 埋め込み層\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_size)\n",
    "        # LSTM\n",
    "        self.lstm = nn.LSTM(self.embedding_size, self.hidden_size, 1, batch_first=True)\n",
    "        # 全結合層\n",
    "        self.linear = nn.Linear(self.hidden_size, self.vocab_size)\n",
    "\n",
    "    def forward(self, seqs):\n",
    "        \"\"\"ターゲット系列を全時刻での入力として出力を計算\n",
    "\n",
    "        Negative Log Likelihoodを計算するために使うので、softmaxではなくlog_softmaxを使います。\n",
    "\n",
    "        :param seqs: torch.Tensor, (batch_size, max_length)\n",
    "        :return outputs: torch.Tensor, (batch_size * max_length, vocab_size)\n",
    "        \"\"\"\n",
    "        N = seqs.size(0) # batch_size\n",
    "\n",
    "        # WRITE ME!\n",
    "        embed = self.embedding(seqs) # (batch_size, max_length, embedding_size)\n",
    "\n",
    "        h, c = self.init_hidden(N) # 隠れ状態ベクトルの初期化\n",
    "        self.lstm.flatten_parameters()\n",
    "\n",
    "        # WRITE ME!\n",
    "        hidden, (h, c) = self.lstm(embed, (h, c)) # hidden:(batch_size, max_length, hidden_size)\n",
    "        lin = self.linear(hidden)  # (batch_size, max_length, vocab_size)\n",
    "\n",
    "        outputs = F.log_softmax(lin, dim=-1) # (batch_size, max_length, vocab_size)\n",
    "        outputs = outputs.view(-1, self.vocab_size) # (batch_size * max_length, vocab_size)\n",
    "        return outputs\n",
    "\n",
    "    def step(self, x, h, c):\n",
    "        \"\"\"時刻をtからt+1に1つだけ進めます\n",
    "\n",
    "        :param x: torch.Tensor, 時刻tの出力かつ時刻t+1の入力\n",
    "        :param h, c: torch.Tensor, 時刻tの隠れ状態ベクトル\n",
    "        :return pred: torch.Tensor, 時刻t+1の出力\n",
    "        :return h, c: torch.Tensor, 時刻t+1の隠れ状態ベクトル\n",
    "        \"\"\"\n",
    "        embed = self.embedding(x) # embed:(batch_size, 1, embedding_size)\n",
    "        self.lstm.flatten_parameters()\n",
    "        hidden, (h, c) = self.lstm(embed, (h, c)) # y:(batch_size, 1, hidden_size)\n",
    "        pred = F.softmax(self.linear(hidden), dim=-1) # (batch_size, 1, vocab_size)\n",
    "        return pred, h, c\n",
    "\n",
    "    def sample(self, x=None):\n",
    "        \"\"\"Generaterでサンプリングするメソッド\n",
    "        x == None (, flag is True)\n",
    "            -> 全系列をサンプリング\n",
    "        x == torch.Tensor, (batch_size, seq_length) (, flag is False)\n",
    "            -> x以降の系列をサンプリング\n",
    "\n",
    "        :param x: None or torch.Tensor, (batch_size, seq_length)\n",
    "        :return output: torch.Tensor, (batch_size, max_length)\n",
    "        \"\"\"\n",
    "        flag = False # 時刻0から始める(True)か否か(False)\n",
    "        if x is None:\n",
    "            flag = True\n",
    "        if flag:\n",
    "            x = torch.empty(self.batch_size, 1).fill_(BOS).long().to(device)\n",
    "        h, c = self.init_hidden(self.batch_size)\n",
    "\n",
    "        samples = []\n",
    "        if flag:\n",
    "            for i in range(self.max_length):\n",
    "                output, h, c = self.step(x, h, c) # output:(batch_size, 1, vocab_size)\n",
    "                output = output.squeeze(1) # (batch_size, vocab_size)\n",
    "                x = output.multinomial(1) # (batch_size, 1), 次の時刻の入力をカテゴリカル分布からサンプリング\n",
    "                samples.append(x)\n",
    "        else:\n",
    "            given_len = x.size(1)\n",
    "            lis = x.chunk(x.size(1), dim=1) # max_length方向に分割\n",
    "            for i in range(given_len): # xを出力し終わった時の隠れ状態ベクトルを再現する\n",
    "                output, h, c = self.step(lis[i], h, c)\n",
    "                samples.append(lis[i])\n",
    "            output = output.squeeze(1)\n",
    "            x = output.multinomial(1)\n",
    "            for i in range(given_len, self.max_length): # モンテカルロ法\n",
    "                samples.append(x)\n",
    "                output, h, c = self.step(x, h, c)\n",
    "                output = output.squeeze(1)\n",
    "                x = output.multinomial(1) # 単語をサンプリング\n",
    "        output = torch.cat(samples, dim=1) # (batch_size, max_length)\n",
    "        return output\n",
    "\n",
    "    def init_hidden(self, N):\n",
    "        \"\"\"LSTMの隠れ状態ベクトルを初期化します。\n",
    "\n",
    "        :param N: int, ミニバッチのサイズ\n",
    "        :return h0, c0: torch.Tensor, 初期状態での隠れ状態ベクトル\n",
    "        \"\"\"\n",
    "        h0 = torch.zeros(1, N, self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(1, N, self.hidden_size).to(device)\n",
    "        return h0, c0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Discriminator\n",
    "与えられたsequenceがGenerated data(Generatorが生成したもの)かTrue dataかを判別するための2値分類器です。\n",
    "モデルはCNNがベースになっていて、内部でHighway Networkを使います。\n",
    "\n",
    "Highway NetworkとはNeural Network内の情報の流れをgateによって制限することで層が多いDeep Neural Networkでも学習を可能にすることができる機構です。\n",
    "\n",
    "<img src=\"./image/discriminator.png\" width=720>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Highway(nn.Module):\n",
    "    \"\"\"Highway Network (cf. http://arxiv.org/abs/1505.00387)\n",
    "\n",
    "    gate = sigmoid(Wx + b)\n",
    "    nonlinear = f(W'x + b')\n",
    "    y = gate * nonlinear + (1 - gate) * x\n",
    "\n",
    "    ここで f はreluなどの非線形な活性化関数です。gateはtransform gate、(1 - gate)はcarry gateと呼ばれます。\n",
    "    xからyを計算する過程がHighway Networkの1層に相当します。\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, num_layers=1, f=F.relu):\n",
    "        \"\"\"\n",
    "        :param input_size: int, 入力のサイズ\n",
    "        :param num_layers: int, Highway Networkの層数\n",
    "        :param f: 非線形な活性化関数\n",
    "        \"\"\"\n",
    "        super(Highway, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.linear1 = nn.ModuleList([nn.Linear(input_size, input_size) for _ in range(num_layers)])\n",
    "        self.linear2 = nn.ModuleList([nn.Linear(input_size, input_size) for _ in range(num_layers)])\n",
    "        self.f = f\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: torch.Tensor, (batch_size, input_size)\n",
    "        :return x: torch.Tensor, (batch_size, input_size)\n",
    "        \"\"\"\n",
    "        for layer in range(self.num_layers):\n",
    "            gate = torch.sigmoid(self.linear1[layer](x))\n",
    "            nonlinear = self.f(self.linear2[layer](x))\n",
    "            x = gate * nonlinear + (1 - gate) * x\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, vocab_size, batch_size, embedding_size,\n",
    "                num_filters, filter_sizes, dropout_prob=0.75):\n",
    "        \"\"\"\n",
    "        :param vocab_size: int, 語彙の総数\n",
    "        :param batch_size: int, ミニバッチのサイズ\n",
    "        :param embedding_size: int, 埋め込みベクトルの次元数\n",
    "        :param num_filters: list of int, CNNのフィルターの数のリスト\n",
    "        :param filter_sizes: list of int, CNNのフィルターのサイズのリスト\n",
    "        :param dropout_prob: float, ドロップアウトの確率\n",
    "        \"\"\"\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.batch_size = batch_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.num_filters = num_filters\n",
    "        self.filter_sizes = filter_sizes\n",
    "        assert len(self.num_filters) == len(self.filter_sizes)\n",
    "        \n",
    "        # 埋め込み層\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_size)\n",
    "        # CNN\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(1, Co, (K, self.embedding_size))\n",
    "                                    for Co, K in zip(self.num_filters, self.filter_sizes)])\n",
    "        # Highway\n",
    "        self.highway = Highway(sum(self.num_filters))\n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        # 全結合層\n",
    "        self.linear = nn.Linear(sum(self.num_filters), 2)\n",
    "\n",
    "    def forward(self, x, log=True):\n",
    "        \"\"\"\n",
    "        :param x: torch.Tensor, (batch_size, max_length)\n",
    "        :param log: bool, log_softmaxを使うかsoftmaxを使うかを決める\n",
    "        :return x: torch.Tensor, (batch_size, 2)\n",
    "        \"\"\"\n",
    "        x = self.embedding(x)  # (batch_size, max_length, embedding_size)\n",
    "        x = x.unsqueeze(1)  # (batch_size, 1, max_length, embedding_size)\n",
    "        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs]  # [(batch_size, Co, Lo), ...]\n",
    "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]  # [(batch_size, Co), ...]\n",
    "        x = torch.cat(x, 1) # (batch_size, sum(self.num_filters))\n",
    "        x = self.highway(x) # (batch_size, sum(self.num_filters))\n",
    "        x = self.dropout(x) # (batch_size, sum(self.num_filters))\n",
    "        x = self.linear(x) # (batch_size, 2)\n",
    "        if log:\n",
    "            x = F.log_softmax(x, dim=-1) # (batch_size, 2)\n",
    "        else:\n",
    "            x = F.softmax(x, dim=-1) # (batch_size, 2) # 真であるか確率、義である確率\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Monte Carlo search→Rewardの計算\n",
    "\n",
    "Monte Carlo (MC) searchとは複数回ランダムに探索を行うことです。囲碁や将棋などで次にどの手を打つのが最も好ましいか考えるときに、数手先まで考えて複数の候補を比較して次に打つべき手を考えるのと同じことです。\n",
    "\n",
    "Rollout policy (ここではGenerator) にMC探索を行わせて得られた複数の系列に対してDiscriminatorが確率を計算し、報酬とします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ROLLOUT():\n",
    "    def __init__(self, model, update_rate):\n",
    "        \"\"\"\n",
    "        :param model: Generatorのインスタンス\n",
    "        :param update_rate: モデルのパラメータの更新率\n",
    "        \"\"\"\n",
    "        self.ori_model = model # Policy: G_\\theta\n",
    "        self.own_model = copy.deepcopy(model) # Rollout policy: G_\\beta\n",
    "        self.update_rate = update_rate # \\alpha\n",
    "\n",
    "    def get_reward(self, x, num, discriminator):\n",
    "        \"\"\"モンテカルロ探索とdiscriminatorを使ってGenerated dataの各時刻に対して報酬（True dataである確率）を与えます。\n",
    "\n",
    "        :param x : torch.Tensor, Generated data, (batch_size, seq_length)\n",
    "        :param num : モンテカルロ法でサンプリングをする回数\n",
    "        :param discriminator : Discrimanatorのインスタンス\n",
    "        :return rewards: torch.Tensor, 報酬（True dataである確率）, (batch_size, seq_length)\n",
    "        \"\"\"\n",
    "        rewards = []\n",
    "        batch_size = x.size(0)\n",
    "        seq_length = x.size(1)\n",
    "        for i in range(num):\n",
    "            for t in range(1, seq_length):\n",
    "                # Y_{1:t}に対して与える報酬を推定\n",
    "                data = x[:, 0:t]\n",
    "                samples = self.own_model.sample(x=data) # Y_{t:T}をモンテカルロ法でサンプリング, (batch_size, seq_length)\n",
    "                pred = discriminator(samples, log=False) # True dataである確率を計算, (batch_size, 2)\n",
    "                pred = pred.cpu().data[:,1].numpy() # (batch_size,)\n",
    "                if i == 0:\n",
    "                    rewards.append(pred)\n",
    "                else:\n",
    "                    rewards[t-1] += pred\n",
    "\n",
    "            # Y_{1:T}に対して与える報酬を推定\n",
    "            pred = discriminator(x, log=False)\n",
    "            pred = pred.cpu().data[:, 1].numpy()\n",
    "            if i == 0:\n",
    "                rewards.append(pred)\n",
    "            else:\n",
    "                rewards[seq_length-1] += pred\n",
    "        rewards = np.array(rewards) # (seq_length, batch_size)\n",
    "        rewards = np.transpose(rewards) / (1.0 * num) # (batch_size, seq_length)\n",
    "        return rewards\n",
    "\n",
    "    def update_params(self):\n",
    "        dic = {}\n",
    "        for name, param in self.own_model.named_parameters():\n",
    "            dic[name] = param.data\n",
    "        for name, param in self.ori_model.named_parameters():\n",
    "            if name.startswith('emb'):\n",
    "                param.data = dic[name]\n",
    "            else:\n",
    "                # パラメータを更新した後と更新する前の内分点をとる(0.3節参照)\n",
    "                param.data = (1 - self.update_rate) * param.data + self.update_rate * dic[name]\n",
    "        self.own_model = copy.deepcopy(self.ori_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 関数定義"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. サンプル生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_samples(model, batch_size, generated_num, output_file):\n",
    "    \"\"\"generatorを使ってサンプル生成を行う関数\n",
    "    \n",
    "    :param model: モデル\n",
    "    :param batch_size: int, バッチサイズ\n",
    "    :param generated_num: int, 生成するサンプルの総数\n",
    "    :param output_file: str, サンプルの出力先ファイル\n",
    "    \"\"\"\n",
    "    samples = []\n",
    "    for _ in range(int(generated_num / batch_size)):\n",
    "        sample = model.sample().cpu().data.numpy().tolist() # (batch_size, max_length)\n",
    "        samples.extend(sample)\n",
    "    with open(output_file, 'w') as fout:\n",
    "        for sample in samples:\n",
    "            string = ' '.join([str(s) for s in sample]) + \"\\n\"\n",
    "            fout.write(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. 訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(model, data_loader, criterion, optimizer=None, is_train=True):\n",
    "    \"\"\"GeneratorまたはDiscriminatorでlossを計算するための関数\n",
    "\n",
    "    :param model: モデル\n",
    "    :param data_loader: モデルのためのデータローダー\n",
    "    :param criterion: nn.NLLLoss()などのlossを計算するクラスのインスタンス\n",
    "    :param optimizer: Adamなどのoptimizer\n",
    "    :param is_train: bool, trainするか否かを決める\n",
    "    :return loss: float\n",
    "    \"\"\"\n",
    "    model.train(is_train)\n",
    "\n",
    "    total_loss = 0.\n",
    "    total_batches = 0.\n",
    "\n",
    "    for batch_X, batch_Y in data_loader:\n",
    "        pred_Y = model(batch_X)\n",
    "        loss = criterion(pred_Y, batch_Y.contiguous().view(-1))\n",
    "        total_loss += loss.item()\n",
    "        total_batches += 1\n",
    "\n",
    "        if is_train:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    data_loader.reset()\n",
    "    loss = total_loss / total_batches\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 実験"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習は以下の流れで行います。\n",
    "* Pretraining（事前学習）\n",
    "  1. GeneratorをTrue dataで事前学習（**教師あり学習**）\n",
    "  1. Discriminatorを教師あり学習\n",
    "\n",
    "を行なった後に\n",
    "\n",
    "* Adversarial Training（敵対的学習）\n",
    "  1. GeneratorをPolicy Gradientで学習（**強化学習**）\n",
    "  1. Discriminatorを教師あり学習\n",
    "  1. 1に戻る"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vocab_size = len(vocab.word2id)\n",
    "generator = Generator(vocab_size, G_BATCH_SIZE, G_EMBEDDING_SIZE, G_HIDDEN_SIZE, G_MAX_LENGTH).to(device)\n",
    "discriminator = Discriminator(vocab_size, D_BATCH_SIZE, D_EMBEDDING_SIZE,\n",
    "                D_NUM_FILTERS, D_FILTER_SIZES, dropout_prob=D_DROPOUT_PROB).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1. Pre-train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1, GenLoss:7.5381\n"
     ]
    }
   ],
   "source": [
    "# pretrain generator\n",
    "gen_data_loader = GenDataLoader(POSITIVE_FILE, G_BATCH_SIZE)\n",
    "gen_criterion = nn.NLLLoss()\n",
    "gen_optimizer = optim.Adam(generator.parameters())\n",
    "for epoch_id in range(1, G_PRE_NUM_EPOCHS + 1):\n",
    "    gen_data_loader.reset()\n",
    "    train_loss = compute_loss(\n",
    "        generator, gen_data_loader, gen_criterion, optimizer=gen_optimizer, is_train=True)\n",
    "    print(\"Epoch:{}, GenLoss:{:.4f}\".format(epoch_id, train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1, DisLoss: 0.3369\n"
     ]
    }
   ],
   "source": [
    "# pretrain discriminator\n",
    "dis_criterion = nn.NLLLoss()\n",
    "dis_optimizer = optim.Adam(discriminator.parameters())\n",
    "for epoch_id in range(1, D_PRE_NUM_EPOCHS + 1):\n",
    "    generate_samples(generator, D_BATCH_SIZE, GENERATED_NUM, NEGATIVE_FILE)\n",
    "    dis_data_loader = DisDataLoader(POSITIVE_FILE, NEGATIVE_FILE, D_BATCH_SIZE)\n",
    "    for _ in range(3):\n",
    "        loss = compute_loss(\n",
    "            discriminator, dis_data_loader, dis_criterion, optimizer=dis_optimizer, is_train=True)\n",
    "        print(\"Epoch:{}, DisLoss: {:.4f}\".format(epoch_id, loss))\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2. Adversarial training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generator Gのパラメータ$\\theta$の更新方法\n",
    "\n",
    "\\begin{align*}\n",
    "& \\theta \\gets \\theta + \\alpha \\nabla J(\\theta) \\\\\n",
    "\\\\\n",
    "& \\nabla J(\\theta) = \\frac{1}{T} \\sum_{t=1}^T {\\mathbb E}_{Y_{1:t} \\sim G_\\theta} \\left[ \\nabla \\log G_\\theta (y_t|Y_{1:t-1}) Q(Y_{1:t-1}, y_t) \\right]\\\\\n",
    "\\\\\n",
    "&Q(Y_{1:t-1}, y_t)=\\\\\n",
    "&\\begin{cases}\n",
    "\\frac{1}{N}\\sum_{n=1}^ND_\\phi(Y^n_{1:T}),~Y^n_{1:T} \\in MC^{G_\\beta}(Y_{1:t};N) & {\\rm for}~ t < T\\\\\n",
    "D_\\phi(Y_{1:t}) & {\\rm for}~t=T\n",
    "\\end{cases}\n",
    "\\\\\n",
    "\\end{align*}\n",
    "\n",
    "${\\mathbb E}_{Y_{1:t} \\sim G_\\theta} \\left[ \\cdot \\right]$は$G_\\theta$を使って$Y_{1:t}$をサンプリングすることで推定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (6661) must match the size of tensor b (2560) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-6e7e5514f26d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# WRITE ME!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlog_prob\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# train_lossにrewardをかけてから平均\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mgen_gan_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (6661) must match the size of tensor b (2560) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "# adversarial training\n",
    "rollout = ROLLOUT(generator, 0.8)\n",
    "\n",
    "gen_gan_criterion = nn.NLLLoss(reduce=False)\n",
    "gen_gan_optimizer = optim.Adam(generator.parameters())\n",
    "dis_criterion = nn.NLLLoss()\n",
    "dis_optimizer = optim.Adam(discriminator.parameters())\n",
    "for batch_id in range(1, ADV_N_BATCHES + 1):\n",
    "    ## Train the generator for one step\n",
    "    for it in range(1):\n",
    "        samples = generator.sample() # (batch_size, max_length)\n",
    "        # generatorへの入力を用意する＝BOSで始まるsequenceを用意する\n",
    "        start_tokens = torch.empty(G_BATCH_SIZE, 1).fill_(BOS).type(torch.long)\n",
    "        start_tokens = start_tokens.to(device) # (batch_size, 1)\n",
    "        inputs = torch.cat([start_tokens, samples],\n",
    "                           dim = 1)[:, :-1].contiguous() # (batch_size, max_length)\n",
    "        targets = samples.contiguous().view((-1,)) # (batch_size * max_length,)\n",
    "        # discriminatorで報酬を計算\n",
    "        rewards = rollout.get_reward(samples, 16, discriminator) # (batch_size, max_length)\n",
    "        rewards = torch.Tensor(rewards).contiguous().view((-1,)).to(device) # (batch_size * max_length,)\n",
    "\n",
    "        # WRITE ME!\n",
    "        log_prob = generator.forward(inputs) # log G_\\theta(), (batch_size * max_length, vocab_size)\n",
    "        train_loss = gen_gan_criterion(log_prob, targets)\n",
    "\n",
    "        # WRITE ME!\n",
    "        train_loss = (log_prob * rewards).mean() # train_lossにrewardをかけてから平均\n",
    "\n",
    "        gen_gan_optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        gen_gan_optimizer.step()\n",
    "\n",
    "    print('Batch: {}, TrainLoss: {:.4f}'.format(batch_id, train_loss.item()))\n",
    "\n",
    "    rollout.update_params()\n",
    "\n",
    "    for _ in range(4):\n",
    "        generate_samples(generator, G_BATCH_SIZE, GENERATED_NUM, NEGATIVE_FILE)\n",
    "        dis_data_loader = DisDataLoader(POSITIVE_FILE, NEGATIVE_FILE, D_BATCH_SIZE)\n",
    "        for _ in range(2):\n",
    "            loss = compute_loss(\n",
    "                discriminator, dis_data_loader, dis_criterion, optimizer=dis_optimizer, is_train=True)\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 出力例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "こちらで事前に学習した時のGeneratorのパラメータと辞書を用意したので、それらを読み込んで実際に文章を生成してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習済みパラメータを読み込む\n",
    "from utils import TextGenerator\n",
    "g = TextGenerator(vocab_size, G_BATCH_SIZE, G_EMBEDDING_SIZE, G_HIDDEN_SIZE, G_MAX_LENGTH).to(device)\n",
    "g.load_state_dict(torch.load(\"./data/trained_generator_params.pth\"))\n",
    "g.eval()\n",
    "samples = g.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習時と同じ辞書を読み込む\n",
    "import pickle\n",
    "with open(\"./data/id2word.pickle\", \"rb\") as f:\n",
    "    id2word = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "私 の 問題 に いう という 自由 で あっ た 。 </S>\n",
      "私 の 駄菓子 盤 を 構成 し た の 背後 で 実現 する って ある 。 </S>\n",
      "私 の 前 に しばしば あっ た 。 </S>\n",
      "私 の です 。 </S>\n",
      "私 の 中 で あっ た 。 </S>\n",
      "帰っ て 黙っ て 下さい 。 </S>\n",
      "私 の 話 を 制する れ です 。 </S>\n",
      "お上 として 、 彼 の 頭 の 意外 に 歌留多 を 拭い た 。 </S>\n",
      "私 の 勉強 、 来 た 。 </S>\n",
      "夕飯 の 親類 の 悪い から も 、 私 の 家 を 開く 事 に 使い まし た 。 </S>\n",
      "私 の 中 が 常に 父 の ため に つい た 。 </S>\n",
      "私 が 乗ら ない 熊 の 哲理 で あっ た 。 </S>\n",
      "拾い読み に なっ た 。 </S>\n",
      "私 の 室 から 出 た 。 </S>\n",
      "私 に 同時に 私 から 、 凝 の 顔 の ある もの で も 私 が 、 湯呑 を 解き ろ 点じ て 出合う を 折り曲げる 。 </S>\n",
      "たった 先生 の 言葉 を 頼ん だ 。 </S>\n",
      "さっさと 断り 誤っ た 私 に は 、 いか 彼 の 心持 を 付く よう に し た 。 </S>\n",
      "私 の 愛し なかっ た 。 </S>\n",
      "私 の 所 で 書物 が 宅 の いる その 後 なら 、 奥さん の 死ぬ の です 。 </S>\n",
      "私 の 私 は そう な の 想像 を 起し た 。 </S>\n",
      "お嬢さん の 叔父 で あっ た 。 </S>\n",
      "私 の 方 から 出よ う 。 </S>\n",
      "私 の 態度 も 時々 、 鈍い お嬢さん に は なかっ た 。 </S>\n",
      "Ｋ から 見れ ば 構わ ない と 推定 し まし た 。 </S>\n",
      "私 の 過去 に 我と を とうから 後らし て 新橋 を 鷹揚 すれ ば 霊 が ある 。 </S>\n",
      "もっとも なぜ 世の中 で なく いっ て 。 </S>\n",
      "私 の 強い し た の です 。 </S>\n",
      "「 私 の 時 、 自分 の 気 に 聞い た 。 </S>\n",
      "それ を 犬 は 大胆 だ 。 </S>\n",
      "Ｋ の 経験 で あっ た 。 </S>\n",
      "頼ま せる の で あっ た 。 </S>\n",
      "私 は 今 の 眼 に 二度と 考え まし た 。 </S>\n",
      "私 の 疑惑 は 、 私 の 年格好 の を 悲しい なくっ た 。 </S>\n",
      "私 の 私 の 苦しい か という 」 奥さん は まだ 厳重 だっ た 。 </S>\n",
      "私 に の 過去 を 驚かし た 。 </S>\n",
      "私 の 許可 を 疑っ た 。 </S>\n",
      "私 の 希望 で あっ た 。 </S>\n",
      "私 の 所 で は なら なかっ た 。 </S>\n",
      "私 の 境遇 に 凝 と 思い たく 思わ れ た 。 </S>\n",
      "私 の 襟 の 書状 が 、 私 の 瞬間 に 、 火鉢 の 修養 が 、 私 の 成立 し た 。 </S>\n",
      "私 の 相違 の 行路 に も ない の よ 。 </S>\n",
      "私 の 枕元 を 繙き 、 どっち か の 見える という 私 の 口 で あっ た 。 </S>\n",
      "私 の 眼 に 講義 の ため に 二 気 に なっ た 。 </S>\n",
      "私 の その 時 私 が 衣食住 も どの ず でも でき て い た 。 </S>\n",
      "余りに Ｋ の 所 へ 来 た 。 </S>\n",
      "彼 の 経験 で 、 また 保た し て くれ まし た 。 </S>\n",
      "私 の 丈夫 に も 、 私 の もの まで 行き ます 。 </S>\n",
      "Ｋ の 私 に 取っ た 。 </S>\n",
      "私 の 私 に は 何 の 眉 を 赧 て み まし た 。 </S>\n",
      "私 の 中 で ある 。 </S>\n",
      "最初 の 顔 を 解釈 し まし た 。 </S>\n",
      "私 の 前 で 、 私 を 旅行 を 洩れ て い た 。 </S>\n",
      "私 に の 医者 で 私 の 枕元 を する の 間 に も 金額 に も 、 私 に どこ か Ｋ に 困ら 下り て 鋭い もの に 反対 する 事 を し た 。 </S>\n",
      "私 の 神経 を ざさ 返書 で 、 若い 話 を し た 。 </S>\n",
      "そうして 奥さん の 唯一 の 意志 の 顔 で 、 一 専門 の 事 を 得 なかっ た 。 </S>\n",
      "私 に は 見舞 に し た 。 </S>\n",
      "彼 の 同級生 で あっ た 。 </S>\n",
      "国 を する と に 財産 の 寝床 で は 、 帰れる という なんて い まし た 。 </S>\n",
      "私 を 読む と 思う の です 。 </S>\n",
      "それとなく いう で あっ た 。 </S>\n",
      "私 の 半分 の 事 で 来ら て あっ た 。 </S>\n",
      "私 の 所 付 で は ない でし た 。 </S>\n",
      "私 の 前 に は 立た ない たび に 善 を 待ち受け た 。 </S>\n",
      "私 の Ｋ を 見せ て い た 。 </S>\n"
     ]
    }
   ],
   "source": [
    "for s in samples.data.cpu().numpy():\n",
    "    print(\" \".join(filter(lambda x: x != \"<PAD>\", [id2word[w] for w in s])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考リンク"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 元論文\n",
    "    - [SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient](https://arxiv.org/abs/1609.05473)\n",
    "- 作者実装(TensorFlow)\n",
    "    - https://github.com/LantaoYu/SeqGAN\n",
    "- 実装(PyTorch)\n",
    "    - https://github.com/ZiJianZhao/SeqGAN-PyTorch\n",
    "\n",
    "\n",
    "- 【強化学習に強くなりたい方向け】\n",
    "    - An Introduction to Reinforcement Learning, Sutton and Barto, 1998\n",
    "        - 強化学習の教科書（英語）、ネットでPDFが公開されてる\n",
    "    - [Reinforcement Learning (DQN) tutorial](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html#)\n",
    "        - PyTorchによる強化学習（DQN）のTutorial\n",
    "\n",
    "\n",
    "- 【意欲のある方向け】次に読むと面白いかもしれない論文リスト\n",
    "    - [Adversarial Feature Matching for Text Generation](https://arxiv.org/abs/1706.03850) in ICML2017\n",
    "        - SeqGANのupdate版\n",
    "    - [Best of Both Worlds: Transferring Knowledge from Discriminative Learning to a Generative Visual Dialog Model](http://papers.nips.cc/paper/6635-best-of-both-worlds-transferring-knowledge-from-discriminative-learning-to-a-generative-visual-dialog-model) in NIPS2017\n",
    "        - Visual Dialog Generation × GAN\n",
    "    - [Improving Neural Machine Translation with Conditional Sequence Generative Adversarial Nets](https://arxiv.org/abs/1703.04887) in NAACL2018\n",
    "        - 機械翻訳 × GAN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
